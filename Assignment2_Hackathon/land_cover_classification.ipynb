{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Land Cover Classification using NDVI Time-Series Data\n",
    "\n",
    "This notebook implements a land cover classification model using NDVI (Normalized Difference Vegetation Index) time-series data. The implementation includes:\n",
    "\n",
    "1. Data preprocessing and feature engineering\n",
    "2. Missing value handling\n",
    "3. Feature scaling\n",
    "4. Model training using Logistic Regression\n",
    "5. Prediction and submission file generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions\n",
    "\n",
    "Below are the helper functions for data preprocessing and feature engineering:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e211c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ndvi_columns(df):\n",
    "    \"\"\"Get NDVI columns from the dataframe.\"\"\"\n",
    "    return [col for col in df.columns if '_N' in col]\n",
    "\n",
    "def preprocess_data(df, is_train=True):\n",
    "    \"\"\"Preprocess the data by handling missing values and engineering features.\"\"\"\n",
    "    \n",
    "    # Get NDVI columns\n",
    "    ndvi_cols = get_ndvi_columns(df)\n",
    "    \n",
    "    # Separate features and target\n",
    "    if is_train:\n",
    "        X = df[ndvi_cols].copy()\n",
    "        y = df['class']\n",
    "    else:\n",
    "        X = df[ndvi_cols].copy()\n",
    "        y = None\n",
    "    \n",
    "    # 1. Handle missing values using median imputation\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(X),\n",
    "        columns=X.columns,\n",
    "        index=X.index\n",
    "    )\n",
    "    \n",
    "    # 2. Calculate rolling statistics (3-point window)\n",
    "    rolling_stats = X_imputed.rolling(window=3, min_periods=1, axis=1)\n",
    "    X_rolling_mean = rolling_stats.mean().fillna(method='ffill').fillna(method='bfill')\n",
    "    X_rolling_std = rolling_stats.std().fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Name the rolling statistics columns\n",
    "    X_rolling_mean.columns = [f'{col}_rolling_mean' for col in X_rolling_mean.columns]\n",
    "    X_rolling_std.columns = [f'{col}_rolling_std' for col in X_rolling_std.columns]\n",
    "    \n",
    "    # 3. Calculate temporal differences\n",
    "    X_diff = X_imputed.diff(axis=1).fillna(0)\n",
    "    X_diff.columns = [f'{col}_diff' for col in X_diff.columns]\n",
    "    \n",
    "    # 4. Calculate global statistics\n",
    "    global_stats = pd.DataFrame(index=X_imputed.index)\n",
    "    global_stats['max_ndvi'] = X_imputed.max(axis=1)\n",
    "    global_stats['min_ndvi'] = X_imputed.min(axis=1)\n",
    "    global_stats['range_ndvi'] = global_stats['max_ndvi'] - global_stats['min_ndvi']\n",
    "    global_stats['mean_ndvi'] = X_imputed.mean(axis=1)\n",
    "    global_stats['std_ndvi'] = X_imputed.std(axis=1)\n",
    "    \n",
    "    # 5. Calculate seasonal features (assuming dates are in chronological order)\n",
    "    n_seasons = 4\n",
    "    season_size = X_imputed.shape[1] // n_seasons\n",
    "    seasonal_stats = pd.DataFrame(index=X_imputed.index)\n",
    "    \n",
    "    for i in range(n_seasons):\n",
    "        start_idx = i * season_size\n",
    "        end_idx = (i + 1) * season_size if i < n_seasons - 1 else X_imputed.shape[1]\n",
    "        season_data = X_imputed.iloc[:, start_idx:end_idx]\n",
    "        seasonal_stats[f'season_{i+1}_mean'] = season_data.mean(axis=1)\n",
    "        seasonal_stats[f'season_{i+1}_std'] = season_data.std(axis=1)\n",
    "    \n",
    "    # Combine all features\n",
    "    features = pd.concat([\n",
    "        X_imputed,        # Original imputed NDVI values\n",
    "        X_rolling_mean,   # Rolling means\n",
    "        X_rolling_std,    # Rolling standard deviations\n",
    "        X_diff,           # Temporal differences\n",
    "        seasonal_stats,   # Seasonal statistics\n",
    "        global_stats      # Global statistics\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Final check for any remaining NaN values\n",
    "    if features.isna().any().any():\n",
    "        features = features.fillna(features.mean())\n",
    "    \n",
    "    return features, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main Execution\n",
    "\n",
    "Load data, train model, and generate predictions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b900a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_df = pd.read_csv('hacktrain.csv')\n",
    "test_df = pd.read_csv('hacktest.csv')\n",
    "\n",
    "print(\"Training data shape:\", train_df.shape)\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "print(\"\\nSample of training data:\")\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2b4264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess training data\n",
    "X_train, y_train = preprocess_data(train_df, is_train=True)\n",
    "\n",
    "# Ensure no NaN values in features\n",
    "if X_train.isna().any().any():\n",
    "    print(\"Filling remaining NaN values in training data...\")\n",
    "    X_train = X_train.fillna(X_train.mean())\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "\n",
    "# Ensure no NaN values after scaling\n",
    "if X_train_scaled.isna().any().any():\n",
    "    print(\"Filling any NaN values after scaling...\")\n",
    "    X_train_scaled = X_train_scaled.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f17bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on training data\n",
    "train_pred = model.predict(X_train_scaled)\n",
    "print(\"\\nTraining Accuracy:\", accuracy_score(y_train, train_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_train, train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e5038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and predict on test data\n",
    "X_test, _ = preprocess_data(test_df, is_train=False)\n",
    "\n",
    "# Ensure no NaN values in test features\n",
    "if X_test.isna().any().any():\n",
    "    print(\"Filling remaining NaN values in test data...\")\n",
    "    X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# Ensure no NaN values after scaling test data\n",
    "if X_test_scaled.isna().any().any():\n",
    "    print(\"Filling any NaN values after scaling test data...\")\n",
    "    X_test_scaled = X_test_scaled.fillna(0)\n",
    "\n",
    "test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'class': test_pred\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"\\nSubmission file created successfully!\")\n",
    "print(submission.head())\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
